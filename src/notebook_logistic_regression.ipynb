{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLSD: Quantised Langevin stochastic dynamics for Bayesian federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "* [Bayesian logistic regression](#section-log)\n",
    "\n",
    "    * [Figure 1 - Synthetic data](#subsection-synthetic)\n",
    "    \n",
    "    * [Figure 2 and Table 2 - Real data](#subsection-real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "import math\n",
    "from torchvision.datasets import EMNIST\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQuantization(s,x): \n",
    "    \n",
    "    # s : number of quantization levels\n",
    "    # x : vector to compress\n",
    "    \n",
    "    if s == 0:\n",
    "        return x\n",
    "    norm_x = np.linalg.norm(x,2)\n",
    "    if norm_x == 0:\n",
    "        return x\n",
    "    ratio = np.abs(x) / norm_x\n",
    "    l = np.floor(ratio * s)\n",
    "    p = ratio * s - l\n",
    "    sampled = np.random.binomial(1,p)\n",
    "    qtzt = np.sign(x) * norm_x * (l + sampled) / s\n",
    "    return qtzt\n",
    "\n",
    "def com_bits(s,d):\n",
    "    if s <= np.sqrt(d/2 - np.sqrt(d)):\n",
    "        return (3 + (3/2) * np.log(2*(s**2 + d)/(s*(s+np.sqrt(d))))) * s*(s+np.sqrt(d)) + 32\n",
    "    elif s == np.sqrt(d):\n",
    "        return 2.8*d + 32\n",
    "    else:\n",
    "        return ((1/2)*(np.log(1 + (s**2+np.minimum(d,s*np.sqrt(d)))/d) + 1) + 2) * d + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum( np.exp(x))\n",
    "    return ex/sum_ex\n",
    "\n",
    "\n",
    "def generate_synthetic(alpha, beta, iid, d, nb_class, b):\n",
    "    \n",
    "    \n",
    "    np.random.seed(1994)\n",
    "    #samples_per_user = np.random.lognormal(4, 2, (b)).astype(int) + 50\n",
    "    samples_per_user = np.random.randint(low=10,high=50,size=b).astype(int)\n",
    "    num_samples = np.sum(samples_per_user)\n",
    "\n",
    "    X_split = [[] for _ in range(b)]\n",
    "    y_split = [[] for _ in range(b)]\n",
    "\n",
    "\n",
    "    #### define some eprior ####\n",
    "    np.random.seed(1994)\n",
    "    mean_W = np.random.normal(0, alpha, b)\n",
    "    mean_b = mean_W\n",
    "    np.random.seed(1994)\n",
    "    B = np.random.normal(0, beta, b)\n",
    "    mean_x = np.zeros((b, d))\n",
    "\n",
    "    diagonal = np.zeros(d)\n",
    "    for j in range(d):\n",
    "        diagonal[j] = np.power((j+1), -1.2)\n",
    "    cov_x = np.diag(diagonal)\n",
    "\n",
    "    for i in range(b):\n",
    "        if iid == 1:\n",
    "            mean_x[i] = np.ones(d) * B[i]  # all zeros\n",
    "        else:\n",
    "            np.random.seed(1994)\n",
    "            mean_x[i] = np.random.normal(B[i], 1, d)\n",
    "\n",
    "    if iid == 1:\n",
    "        np.random.seed(1994)\n",
    "        W_global = np.random.normal(0, 1, (d, nb_class))\n",
    "        np.random.seed(1994)\n",
    "        b_global = np.random.normal(0, 1,  nb_class)\n",
    "\n",
    "    for i in range(b):\n",
    "        np.random.seed(1994)\n",
    "        W = np.random.normal(mean_W[i], 1, (d, nb_class))\n",
    "        np.random.seed(1994)\n",
    "        b = np.random.normal(mean_b[i], 1,  nb_class)\n",
    "\n",
    "        if iid == 1:\n",
    "            W = W_global\n",
    "            b = b_global\n",
    "        np.random.seed(1994)\n",
    "        xx = np.random.multivariate_normal(mean_x[i], cov_x, samples_per_user[i])\n",
    "        yy = np.zeros(samples_per_user[i])\n",
    "\n",
    "        for j in range(samples_per_user[i]):\n",
    "            tmp = np.dot(xx[j], W) + b\n",
    "            yy[j] = np.argmax(softmax(tmp))\n",
    "\n",
    "        X_split[i] = xx.tolist()\n",
    "        y_split[i] = yy.tolist()\n",
    "\n",
    "    return X_split, y_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian logistic regression  <a class=\"anchor\" id=\"section-log\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALA(T_total,T_bi,T,gamma,init):\n",
    "    \n",
    "    # Definition of the arrays of interest\n",
    "    loss = np.zeros(T)\n",
    "    acc = np.zeros(T)\n",
    "    theta = np.zeros((T,d))\n",
    "    \n",
    "    # Initialisation\n",
    "    theta[0,:] = init\n",
    "    x = theta[0,:]\n",
    "    P = 1 / (1 + np.exp(-np.dot(X_tot,theta[0,:])))\n",
    "    P = P - 1e-5\n",
    "    P[P<0] = 1e-10\n",
    "    loss[0] = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) + (tau/2) * np.linalg.norm(x)**2\n",
    "    \n",
    "    # Parameters\n",
    "    thin = int((T_total-T_bi+1)/T)\n",
    "\n",
    "    # MCMC sampler    \n",
    "    for t in range(T_total-1): # total number of iterations\n",
    "        \n",
    "        P = 1 / (1 + np.exp(-np.dot(X_tot,x)))\n",
    "        grad = np.dot(X_tot.T, np.reshape(P - y_tot,(np.sum(n),))) + tau * x\n",
    "        grad = np.reshape(grad,(d,))\n",
    "        s = np.zeros(d)\n",
    "        s = x - gamma * grad + np.sqrt(2*gamma) * np.random.normal(0,1,d)\n",
    "    \n",
    "        u = np.random.uniform()\n",
    "        P = 1 / (1 + np.exp(-np.dot(X_tot,s)))\n",
    "        U = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) + tau/2 * np.linalg.norm(s)**2\n",
    "        P = 1 / (1 + np.exp(-np.dot(X_tot,x)))\n",
    "        U_old = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) + tau/2 * np.linalg.norm(x)**2\n",
    "        P = 1 / (1 + np.exp(-np.dot(X_tot,s)))\n",
    "        grad_prop = np.dot(X_tot.T, np.reshape(P - y_tot,(np.sum(n),))) + tau * s\n",
    "        grad_prop = np.reshape(grad,(d,))\n",
    "        ratio = np.exp(-U + U_old \\\n",
    "                       - (1/(4*gamma))*np.linalg.norm(x - s + gamma * grad_prop,2)**2 \\\n",
    "                       + (1/(4*gamma))*np.linalg.norm(s - x + gamma * grad,2)**2)\n",
    "        #print(min(1,ratio))\n",
    "        if u <= ratio:\n",
    "            x = s\n",
    "          \n",
    "        # Save theta according to the thinning strategy\n",
    "        if (t % thin == 0) and (t >= T_bi):\n",
    "            theta[int((t-T_bi)/thin) + 1,:] = x\n",
    "            P = 1 / (1 + np.exp(-np.dot(X_tot,x)))\n",
    "            P = P - 1e-5\n",
    "            P[P<0] = 1e-10\n",
    "            loss[int((t-T_bi)/thin) + 1] = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) + (tau/2) * np.linalg.norm(x)**2\n",
    "            \n",
    "            # Accuracy\n",
    "            a = np.exp(X_tot.dot(x)) / (1 + np.exp(X_tot.dot(x)))\n",
    "            a[a >= 0.5] = 1\n",
    "            a[a < 0.5] = 0\n",
    "            err = np.sum(np.abs(a - y_tot))/len(y_tot)\n",
    "            acc[int((t-T_bi)/thin) + 1] = 1 - err\n",
    "        \n",
    "        if t % 100000 == 0:\n",
    "            print(t)\n",
    "            \n",
    "    return theta, loss, acc\n",
    "\n",
    "def QLSDpp(T_total,T_bi,T,gamma,p,init,s,K,beta,memory):\n",
    "    \n",
    "    # Definition of the arrays of interest\n",
    "    loss = np.zeros(T)\n",
    "    theta = np.zeros((T,d))\n",
    "    qgrad = np.zeros((b,d))\n",
    "    h = np.zeros((b,d))\n",
    "    \n",
    "    # Initialisation\n",
    "    theta[0,:] = init\n",
    "    x = init\n",
    "    P = 1 / (1 + np.exp(-np.dot(X_tot,theta[0,:])))\n",
    "    P = P - 1e-5\n",
    "    P[P<0] = 1e-10\n",
    "    loss[0] = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) + (tau/2) * np.linalg.norm(theta[0,:])**2\n",
    "    z = theta[0,:]\n",
    "    \n",
    "    # Parameters\n",
    "    thin = int((T_total-T_bi+1)/T)\n",
    "\n",
    "    # MCMC sampler    \n",
    "    for t in range(T_total-1): # total number of iterations\n",
    "        \n",
    "        if t % K == 0: # update of the control variate every K iterations\n",
    "            z = x\n",
    "\n",
    "        for i in range(b): # for each client in parallel\n",
    "            \n",
    "             # Compute full gradient at control variate\n",
    "            P = 1 / (1 + np.exp(-np.dot(X[i],z)))\n",
    "            fullgrad_z = -np.dot(X[i].T, np.reshape(y[i] - P,(n[i],))) + tau * n[i]/np.sum(n) * z\n",
    "            fullgrad_z = np.reshape(fullgrad_z,(d,))\n",
    "\n",
    "            # Compute stochastic gradient based on SVRG scheme\n",
    "            idx = np.random.randint(0, n[i])\n",
    "            grad_theta = (1 / (1 + np.exp(-np.dot(X[i][idx,:],x))) - y[i][idx]) * X[i][idx,:] \\\n",
    "                         + tau * (1/np.sum(n)) * x\n",
    "            grad_z = (1 / (1 + np.exp(-np.dot(X[i][idx,:],z))) - y[i][idx]) * X[i][idx,:] \\\n",
    "                         + tau * (1/np.sum(n)) * z\n",
    "            grad = n[i] * (grad_theta - grad_z) + fullgrad_z\n",
    "\n",
    "            # Quantize the stochastic gradient minus memory term\n",
    "            qqgrad = SQuantization(s, grad - h[i,:])\n",
    "            qgrad[i,:] = qqgrad + h[i,:]\n",
    "            if memory == True:\n",
    "                h[i,:] = h[i,:] + beta * qqgrad   \n",
    "        \n",
    "        \n",
    "        # Update theta on server\n",
    "        x = x - gamma * np.sum(qgrad,axis=0) + np.sqrt(2*gamma) * np.random.normal(0,1,size=d)\n",
    "    \n",
    "        # Save theta according to the thinning strategy\n",
    "        if (t % thin == 0) and (t >= T_bi):\n",
    "            theta[int((t-T_bi)/thin) + 1,:] = x\n",
    "            P = 1 / (1 + np.exp(-np.dot(X_tot,x)))\n",
    "            P = P - 1e-5\n",
    "            P[P<0] = 1e-10\n",
    "            loss[int((t-T_bi)/thin) + 1] = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) \\\n",
    "                                           + (tau/2) * np.linalg.norm(x)**2\n",
    "            \n",
    "    return theta, loss\n",
    "\n",
    "def LSDpp(T_total,T_bi,T,gamma,p,init,K):\n",
    "    \n",
    "    # Definition of the arrays of interest\n",
    "    loss = np.zeros(T)\n",
    "    theta = np.zeros((T,d))\n",
    "    grad = np.zeros((b,d))\n",
    "    \n",
    "    # Initialisation\n",
    "    theta[0,:] = init\n",
    "    x = init\n",
    "    P = 1 / (1 + np.exp(-np.dot(X_tot,theta[0,:])))\n",
    "    P = P - 1e-5\n",
    "    P[P<0] = 1e-10\n",
    "    loss[0] = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) + (tau/2) * np.linalg.norm(theta[0,:])**2\n",
    "    z = theta[0,:]\n",
    "    \n",
    "    # Parameters\n",
    "    thin = int((T_total-T_bi+1)/T)\n",
    "\n",
    "    # MCMC sampler    \n",
    "    for t in range(T_total-1): # total number of iterations\n",
    "        \n",
    "        if t % K == 0: # update of the control variate every K iterations\n",
    "            z = x\n",
    "\n",
    "        for i in range(b): # for each client in parallel\n",
    "            \n",
    "             # Compute full gradient at control variate\n",
    "            P = 1 / (1 + np.exp(-np.dot(X[i],z)))\n",
    "            fullgrad_z = -np.dot(X[i].T, np.reshape(y[i] - P,(n[i],))) + tau * n[i]/np.sum(n) * z\n",
    "            fullgrad_z = np.reshape(fullgrad_z,(d,))\n",
    "\n",
    "            # Compute stochastic gradient based on SVRG scheme\n",
    "            idx = np.random.randint(0, n[i])\n",
    "            grad_theta = (1 / (1 + np.exp(-np.dot(X[i][idx,:],x))) - y[i][idx]) * X[i][idx,:] \\\n",
    "                         + tau * (1/np.sum(n)) * x\n",
    "            grad_z = (1 / (1 + np.exp(-np.dot(X[i][idx,:],z))) - y[i][idx]) * X[i][idx,:] \\\n",
    "                         + tau * (1/np.sum(n)) * z\n",
    "            grad[i,:] = n[i] * (grad_theta - grad_z) + fullgrad_z \n",
    "        \n",
    "        \n",
    "        # Update theta on server\n",
    "        x = x - gamma * np.sum(grad,axis=0) + np.sqrt(2*gamma) * np.random.normal(0,1,size=d)\n",
    "    \n",
    "        # Save theta according to the thinning strategy\n",
    "        if (t % thin == 0) and (t >= T_bi):\n",
    "            theta[int((t-T_bi)/thin) + 1,:] = x\n",
    "            P = 1 / (1 + np.exp(-np.dot(X_tot,x)))\n",
    "            P = P - 1e-5\n",
    "            P[P<0] = 1e-10\n",
    "            loss[int((t-T_bi)/thin) + 1] = - np.dot(y_tot,np.log(P)) - np.dot(1-y_tot,np.log(1-P)) \\\n",
    "                                           + (tau/2) * np.linalg.norm(x)**2\n",
    "            \n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Synthetic data <a class=\"anchor\" id=\"subsection-synthetic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 20 # number of clients\n",
    "nb_class = 2 # number of classes\n",
    "d = 10 # dimension\n",
    "\n",
    "train_data = {'users': [], 'user_data':{}, 'num_samples':[]}\n",
    "test_data = {'users': [], 'user_data':{}, 'num_samples':[]}\n",
    "\n",
    "\n",
    "X, y = generate_synthetic(alpha=1, beta=1, iid=0, d=d, nb_class=nb_class, b=b)     # synthetic (1,1)\n",
    "\n",
    "\n",
    "# Create data structure\n",
    "train_data = {'users': [], 'user_data':{}, 'num_samples':[]}\n",
    "test_data = {'users': [], 'user_data':{}, 'num_samples':[]}\n",
    "    \n",
    "for i in trange(b, ncols=120):\n",
    "\n",
    "    uname = 'f_{0:05d}'.format(i)        \n",
    "    combined = list(zip(X[i], y[i]))\n",
    "    np.random.shuffle(combined)\n",
    "    X[i][:], y[i][:] = zip(*combined)\n",
    "    num_samples = len(X[i])\n",
    "    train_len = int(0.9 * num_samples)\n",
    "    test_len = num_samples - train_len\n",
    "        \n",
    "    train_data['users'].append(uname) \n",
    "    train_data['user_data'][uname] = {'x': X[i][:train_len], 'y': y[i][:train_len]}\n",
    "    train_data['num_samples'].append(train_len)\n",
    "    test_data['users'].append(uname)\n",
    "    test_data['user_data'][uname] = {'x': X[i][train_len:], 'y': y[i][train_len:]}\n",
    "    test_data['num_samples'].append(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the observations vector y\n",
    "y = []\n",
    "for i in range(b):\n",
    "    a = np.reshape(train_data['user_data'][train_data['users'][i]]['y'],(n[i],))\n",
    "    a[a == -1] = 0\n",
    "    y.append(a)\n",
    "    \n",
    "# Define the observation matrix X\n",
    "X = []\n",
    "for i in range(b):\n",
    "    a = np.reshape(train_data['user_data'][train_data['users'][i]]['x'],(n[i],d))\n",
    "    X.append(a)\n",
    "    \n",
    "# Define total observations and features\n",
    "y_tot = np.concatenate(y,axis=0)\n",
    "X_tot = np.concatenate(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "w, v = np.linalg.eig(X_tot.T.dot(X_tot))\n",
    "w = np.max(w.real)\n",
    "tau = 1000\n",
    "M = w/4 + tau\n",
    "S = [2**1,2**2,2**4]\n",
    "K = 100\n",
    "gamma = 1/(M)\n",
    "omega = [np.minimum(d/s**2,np.sqrt(d)/s) for s in S]\n",
    "T = 100000\n",
    "\n",
    "thetaMALA, lossMALA, acc = MALA(T_total=T,T_bi=0,T=T,gamma=1/(10*M),init=np.zeros(d))\n",
    "\n",
    "# Repetitions\n",
    "rep = 20\n",
    "\n",
    "# Init.\n",
    "theta1 = np.zeros((rep,T,d))\n",
    "theta2 = np.zeros((rep,T,d))\n",
    "theta3 = np.zeros((rep,T,d))\n",
    "theta4 = np.zeros((rep,T,d))\n",
    "theta5 = np.zeros((rep,T,d))\n",
    "theta6 = np.zeros((rep,T,d))\n",
    "theta7 = np.zeros((rep,T,d))\n",
    "\n",
    "for r in range(rep):\n",
    "    \n",
    "    np.random.seed(r)\n",
    "\n",
    "    theta1[r,:],loss1 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),s=S[0],K=K,beta=1/(omega[0]+1),memory=True)\n",
    "    theta2[r,:],loss2 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),s=S[0],K=K,beta=1/(omega[0]+1),memory=False)\n",
    "    theta3[r,:],loss3 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),s=S[1],K=K,beta=1/(omega[1]+1),memory=True)\n",
    "    theta4[r,:],loss4 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),s=S[1],K=K,beta=1/(omega[1]+1),memory=False)\n",
    "    theta5[r,:],loss5 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),s=S[2],K=K,beta=1/(omega[2]+1),memory=True)\n",
    "    theta6[r,:],loss6 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),s=S[2],K=K,beta=1/(omega[2]+1),memory=False)\n",
    "    theta7[r,:],loss7 = LSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=0.1,init=np.zeros(d),K=K)\n",
    "    \n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Real data <a class=\"anchor\" id=\"subsection-real\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EMNIST('./data', split=\"digits\", train=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_dataset = EMNIST('./data', split=\"digits\", train=False, transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "def femnist_star(dataset, num_users):\n",
    "    \"\"\"\n",
    "        Sample non-IID client data from EMNIST dataset -> FEMNIST*\n",
    "        :param dataset:\n",
    "        :param num_users:\n",
    "        :return: dict of image index\n",
    "    \"\"\"\n",
    "    print(\"Sampling dataset: FEMNIST*\")\n",
    "    np.random.seed(1994)\n",
    "\n",
    "    dict_users = {i: [] for i in range(num_users)}\n",
    "    total_len = len(dataset)\n",
    "\n",
    "    labels = dataset.targets.numpy()\n",
    "    idxs = np.argsort(labels)\n",
    "\n",
    "    num_shards, num_imgs = 26 * num_users, total_len // (num_users * 26)\n",
    "\n",
    "    label_selected = [np.random.choice(26, 20, replace=False) for _ in range(num_users)]\n",
    "\n",
    "    label_selected_1 = [np.random.choice(label_selected[i], 6, replace=False) for i in range(num_users)]\n",
    "    for i in range(num_users):\n",
    "        for j in label_selected[i]:\n",
    "            ind_pos = np.random.choice(num_users)\n",
    "            tmp = copy.deepcopy(idxs[j * num_users * num_imgs + ind_pos * num_imgs: j * num_users * num_imgs + (ind_pos + 1) * num_imgs])\n",
    "            dict_users[i].append(tmp)\n",
    "        for j in label_selected_1[i]:\n",
    "            ind_pos = np.random.choice(num_users)\n",
    "            tmp = copy.deepcopy(idxs[j * num_users * num_imgs + ind_pos * num_imgs: j * num_users * num_imgs + (\n",
    "                        ind_pos + 1) * num_imgs])\n",
    "            dict_users[i].append(tmp)\n",
    "\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = np.concatenate(tuple(dict_users[i]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 10 # number of clients\n",
    "n_max = 100 # maximum number of observations on each client\n",
    "n_min = 20 # minimum number of observations on each client\n",
    "np.random.seed(1994)\n",
    "n = np.random.randint(low=n_min,high=n_max,size=b) # number of observations per client\n",
    "\n",
    "d = 28*28\n",
    "\n",
    "dict_users = femnist_star(train_dataset,b)\n",
    "X = []\n",
    "y = []\n",
    "for i in range(b):\n",
    "    np.random.seed(1994)\n",
    "    idx_client = np.random.choice(dict_users[i],n[i],replace=False)\n",
    "    X.append([np.reshape(train_dataset[i][0].numpy(),(d,)) for i in idx_client])\n",
    "    y.append([train_dataset[i][1] for i in idx_client])\n",
    "\n",
    "# Define total observations and features\n",
    "y_tot = np.concatenate(y,axis=0)\n",
    "X_tot = np.concatenate(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "K = 10\n",
    "tau = 100\n",
    "w, v = np.linalg.eig(np.dot(X_tot.T,X_tot))\n",
    "w = np.max(w.real)\n",
    "M = w * (1/4) + tau \n",
    "gamma = 1/(5*M)\n",
    "K = 100\n",
    "p = 0.2\n",
    "S = [2**4,2**8,2**16]\n",
    "omega = [np.minimum(d/s**2,np.sqrt(d)/s) for s in S]\n",
    "init = np.random.normal(0,1,size=d)\n",
    "T = 500000\n",
    "\n",
    "theta1,loss1,acc1 = LSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=p,init=init,L=L)\n",
    "theta2,loss2,acc2 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=p,init=init,s=S[0],K=K,alpha=1/(omega[1]+1))\n",
    "theta3,loss3,acc3 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=p,init=init,s=S[1],K=K,alpha=1/(omega[2]+1))\n",
    "theta4,loss4,acc4 = QLSDpp(T_total=T,T_bi=0,T=T,gamma=gamma,p=p,init=init,s=S[2],K=K,alpha=1/(omega[3]+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HPD error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "Tbi = 50000\n",
    "alpha = 0.01\n",
    "beta = 0.99\n",
    "eta1 = np.zeros(N)\n",
    "eta2 = np.zeros(N)\n",
    "eta3 = np.zeros(N)\n",
    "eta4 = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    eta1[i] = np.quantile(loss1[Tbi:],np.linspace(alpha,beta,N)[i])\n",
    "    eta2[i] = np.quantile(loss2[Tbi:],np.linspace(alpha,beta,N)[i])\n",
    "    eta3[i] = np.quantile(loss3[Tbi:],np.linspace(alpha,beta,N)[i])\n",
    "    eta4[i] = np.quantile(loss4[Tbi:],np.linspace(alpha,beta,N)[i])\n",
    "    \n",
    "print((np.abs(eta2-eta1)/eta1)[99])\n",
    "print((np.abs(eta3-eta1)/eta1)[99])\n",
    "print((np.abs(eta4-eta1)/eta1)[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive distribution on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.reshape(test_dataset[i][0].numpy(),(d,)) for i in range(1000)]\n",
    "y = [test_dataset[i][1] for i in range(1000)]\n",
    "\n",
    "idx = [0,1,2,31,5,11,15,17]\n",
    "\n",
    "T = 100000\n",
    "Tbi = 500000 - T \n",
    "K = 10\n",
    "probaLSD = np.zeros((len(idx),T,K))\n",
    "probaQLSD = np.zeros((len(idx),T,K))\n",
    "for t in range(T):\n",
    "    for i in range(len(idx)):\n",
    "        AQLSD = np.reshape(np.exp(np.dot(X[idx[i]],theta2[Tbi+t,:])),(1,K))\n",
    "        ALSD = np.reshape(np.exp(np.dot(X[idx[i]],theta1[Tbi+t,:])),(1,K))\n",
    "        for k in range(K):\n",
    "            probaQLSD[i,t,k] = np.exp(np.dot(X[idx[i]],theta2[Tbi+t,:,k])) / np.sum(AQLSD,axis=1)\n",
    "            probaLSD[i,t,k] = np.exp(np.dot(X[idx[i]],theta1[Tbi+t,:,k])) / np.sum(ALSD,axis=1)\n",
    "            \n",
    "indicesMax = np.zeros(len(idx))\n",
    "for i in range(len(idx)):\n",
    "    indicesMax[i] = np.reshape(np.where(np.mean(probaQLSD[i,:],axis=0) \\\n",
    "                                        == np.max(np.mean(probaQLSD[i,:],axis=0))),(1,))\n",
    "\n",
    "# QLSD\n",
    "df = pd.DataFrame(data=probaQLSD[0,:,int(indicesMax[0])],\n",
    "                  columns=['Predictive distribution of most probable label'])\n",
    "df['Algorithm'] = 'QLSD++ 4 bits'\n",
    "df['observation'] = 0\n",
    "for i in range(len(idx)-1):\n",
    "    i += 1\n",
    "    df1 = pd.DataFrame(data=probaQLSD[i,:,int(indicesMax[i])],\n",
    "                       columns=['Predictive distribution of most probable label'])\n",
    "    df1['Algorithm'] = 'QLSD++ 4 bits'\n",
    "    df1['observation'] = i\n",
    "    df = pd.concat([df,df1])\n",
    "\n",
    "dfLSD = pd.DataFrame(data=probaLSD[0,:,int(indicesMax[0])],\n",
    "                     columns=['Predictive distribution of most probable label'])\n",
    "dfLSD['Algorithm'] = 'LSD++'\n",
    "dfLSD['observation'] = 0\n",
    "for i in range(len(idx)-1):\n",
    "    i += 1\n",
    "    df1 = pd.DataFrame(data=probaLSD[i,:,int(indicesMax[i])],\n",
    "                       columns=['Predictive distribution of most probable label'])\n",
    "    df1['Algorithm'] = 'LSD++'\n",
    "    df1['observation'] = i\n",
    "    dfLSD = pd.concat([dfLSD,df1])\n",
    "\n",
    "df = pd.concat([df,dfLSD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "g = sns.boxplot(x = df['observation'], \n",
    "            y = df['Predictive distribution of most probable label'],\n",
    "            hue = df['Algorithm'],\n",
    "            whis=[5, 95])\n",
    "g.set(xticklabels=[])  \n",
    "g.set(xlabel=None)\n",
    "plt.legend(bbox_to_anchor=(0.1, 0.35), loc=2, borderaxespad=0.);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
